{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JwyynUiBWzZp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from pprint import pprint\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2p_bfde0XIaH"
   },
   "source": [
    "## Importing Dataset (War and Peace - Leo Tolstoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt -O text1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head text1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MF0UDI2LZHGo",
    "outputId": "699d4fe3-b683-4d58-b84c-e53479ab33c7"
   },
   "outputs": [],
   "source": [
    "with open('text1.txt','r') as file:\n",
    "  text1=file.read()      # converting file to string\n",
    "print(len(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8JAj5joYIj0"
   },
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NPzDFS_4Ei2w"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "  text=text.lower()\n",
    "  text = re.sub(r'\\.{3,}', '.', text)\n",
    "  text = re.sub(r'\\n\\s*\\n', ' ' + '.' * 5 + ' ', text)\n",
    "  text=re.sub(r'(\\w)\\n(\\w)',r'\\1 \\2',text)\n",
    "  text=re.sub(r'[^a-zA-Z0-9 \\'\\.]',' ',text)\n",
    "  text=re.sub(r'[\\']','',text)\n",
    "  text = text.replace('\\n', ' ')\n",
    "  cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "  return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "Bn7J4q1cYhm8",
    "outputId": "f96dfcb9-f991-4b8d-c851-73a8e6f15ab7"
   },
   "outputs": [],
   "source": [
    "clean_text1=clean_text(text1)\n",
    "clean_text1[:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfGEr4FkmaV4"
   },
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "X2x9q9Xo0LyM"
   },
   "outputs": [],
   "source": [
    "def paragraph_processing(text, context_len):\n",
    "\n",
    "  context_padding = '.' * context_len\n",
    "  paragraphs=text.split(\".....\")\n",
    "  processed_paragraphs = [context_padding + para.strip() for para in paragraphs]\n",
    "\n",
    "  return processed_paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZFm_WA50TnK",
    "outputId": "aeeca468-2aff-48ee-89f9-35305cf5f7b2"
   },
   "outputs": [],
   "source": [
    "context_len=5\n",
    "paragraphs_txt1=paragraph_processing(clean_text1,context_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "EmS1_GB80aQz"
   },
   "outputs": [],
   "source": [
    "def tokenization(paragraphs_txt,context_len):\n",
    "  tokens=[]\n",
    "  for para in paragraphs_txt:\n",
    "    para_tokens = re.findall(r'\\b\\w+\\b|\\.{' + str(context_len) + r'}|[.]', para)\n",
    "    para_tokens = [token for token in para_tokens if token != '.' * context_len]\n",
    "    tokens.extend(para_tokens)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FkgaQq2q0cGK"
   },
   "outputs": [],
   "source": [
    "tokens_txt1=tokenization(paragraphs_txt1,context_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CApK3MS9kj5j"
   },
   "source": [
    "#### Creating Word Vocabulary and mappings to/from integer indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zYaD5wKimRea"
   },
   "outputs": [],
   "source": [
    "def create_vocab(tokens):\n",
    "    token_to_index = {\n",
    "      '.': 0,\n",
    "      ' ': 1,\n",
    "    }\n",
    "    unique_tokens = sorted(list(set(token for token in tokens if token not in token_to_index)))\n",
    "    token_to_index.update({token: idx + 2 for idx, token in enumerate(unique_tokens)})\n",
    "    index_to_token = {idx: token for token, idx in token_to_index.items()}\n",
    "\n",
    "    return token_to_index, index_to_token, unique_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "LUnNyQ9_Luiq"
   },
   "outputs": [],
   "source": [
    "token_to_index1, index_to_token1, unique_tokens1 = create_vocab(tokens_txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CY3r-Fw7Bg65",
    "outputId": "c2ea1d4f-d387-43a8-cfc3-3af9c54d4999"
   },
   "outputs": [],
   "source": [
    "print(len(unique_tokens1))\n",
    "print(len(list(token_to_index1.items())))\n",
    "print(len(list(index_to_token1.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKqS8SEPoKEC"
   },
   "source": [
    "## Creating X,y Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ody5jRJzBjWE"
   },
   "outputs": [],
   "source": [
    "def create_X_y(paragraphs, token_to_index, index_to_token, context_len):\n",
    "    X = []\n",
    "    y = []\n",
    "    for para in paragraphs:\n",
    "      para_tokens = re.findall(r'\\b\\w+\\b|[.]', para)\n",
    "      if len(para_tokens) <= context_len:\n",
    "        continue\n",
    "      for i in range(len(para_tokens) - context_len):\n",
    "          input_context = [token_to_index[token] for token in para_tokens[i:i + context_len]]\n",
    "          output_word = token_to_index[para_tokens[i + context_len]]\n",
    "          X.append(input_context)\n",
    "          y.append(output_word)\n",
    "\n",
    "          print(' '.join(index_to_token[i] for i in input_context),' ------> ',index_to_token[output_word])\n",
    "\n",
    "    print('Training Samples No. : ', len(X))\n",
    "    print('Training outputs no. : ', len(y))\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JW4VdTy-EIrm",
    "outputId": "9c216bc7-e83f-4cf7-8be2-d05855bc5f75"
   },
   "outputs": [],
   "source": [
    "X1, y1= create_X_y(paragraphs_txt1, token_to_index1, index_to_token1, context_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DI27OuxRdkWZ"
   },
   "outputs": [],
   "source": [
    "subset_size_1 = len(X1) // 7\n",
    "X1_subset = X1[:subset_size_1]\n",
    "y1_subset = y1[:subset_size_1]\n",
    "\n",
    "print(len(X1_subset))\n",
    "print(X1_subset[:10])\n",
    "print(len(y1_subset))\n",
    "print(y1_subset[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sReM0-a2grsB"
   },
   "source": [
    "## Embedding and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "4vqF8V6-hG7Z"
   },
   "outputs": [],
   "source": [
    "emb_dim=128\n",
    "hidden_layer_size=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "erL78l4fg4TE"
   },
   "outputs": [],
   "source": [
    "class NextTokenGen(nn.Module):\n",
    "  def __init__(self, context_len, vocab_size, emb_dim, hidden_layer_size):\n",
    "    super(NextTokenGen,self).__init__()\n",
    "    self.context_len = context_len\n",
    "    self.emb_dim = emb_dim\n",
    "    self.embed=nn.Embedding(vocab_size,emb_dim)\n",
    "    self.layer0=nn.Linear(context_len*emb_dim, hidden_layer_size)\n",
    "    self.layer1=nn.Linear(hidden_layer_size, vocab_size)\n",
    "\n",
    "  def forward(self, X, activation=None):\n",
    "    X=self.embed(X)\n",
    "    X=X.view(X.shape[0],self.context_len*self.emb_dim)\n",
    "    if activation=='relu':\n",
    "      X=F.relu(self.layer0(X))\n",
    "    elif activation=='tanh':\n",
    "      X=torch.tanh(self.layer0(X))\n",
    "    else:\n",
    "      X=self.layer0(X)\n",
    "\n",
    "    X=self.layer1(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "JGOXMZVkGyxs"
   },
   "outputs": [],
   "source": [
    "text_gen1 = NextTokenGen(context_len,len(list(token_to_index1.items())),emb_dim,hidden_layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "HVuwl5KAKRXN"
   },
   "outputs": [],
   "source": [
    "def model_training(model, batch_size, epoch_no, learn_rate, X, y, act_fn):\n",
    "\n",
    "  loss_fn=nn.CrossEntropyLoss()\n",
    "  optimizer=torch.optim.AdamW(model.parameters(), lr=learn_rate)\n",
    "\n",
    "  for epoch in range(epoch_no):\n",
    "    epoch_loss=0.0\n",
    "\n",
    "    for i in range(0,X.shape[0],batch_size):\n",
    "      optimizer.zero_grad()\n",
    "      X_batch=X[i:i+batch_size]\n",
    "      y_batch=y[i:i+batch_size]\n",
    "      y_pred=model(X_batch, activation=act_fn)\n",
    "      loss=loss_fn(y_pred,y_batch)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      epoch_loss+=loss.item()\n",
    "\n",
    "    epoch_loss = epoch_loss / (X.shape[0] // batch_size)\n",
    "\n",
    "    if epoch%10==0:\n",
    "      print(f\"Epoch-{epoch} loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwarN6uMYDu5"
   },
   "outputs": [],
   "source": [
    "model_training(text_gen1,200,250,0.005,X1_subset,y1_subset,'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSRfHLI-sZM3"
   },
   "source": [
    "## Saving The Model Using Pickle To Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "8J1KxipEdstp"
   },
   "outputs": [],
   "source": [
    "def save_model_to_drive(model, model_name: str):\n",
    "\n",
    "  drive.mount('/content/drive')\n",
    "  os.makedirs('/content/drive/MyDrive/checkpoints', exist_ok=True)\n",
    "  print(os.listdir('/content/drive/MyDrive/checkpoints'))\n",
    "\n",
    "  model_path = f'/content/drive/MyDrive/checkpoints/{model_name}.pkl'\n",
    "\n",
    "  with open(model_path, 'wb') as f:\n",
    "      pickle.dump(model, f)\n",
    "\n",
    "  print(f'Model saved to /content/drive/MyDrive/checkpoints/{model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "McPfIatOnzyN"
   },
   "outputs": [],
   "source": [
    "def load_model_from_drive(model_name: str):\n",
    "\n",
    "  drive.mount('/content/drive')\n",
    "  with open(f'/content/drive/MyDrive/checkpoints/{model_name}.pkl', 'rb') as f:\n",
    "    model_loaded = pickle.load(f)\n",
    "\n",
    "  print('Model loaded successfully!')\n",
    "  return model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaA9fhFLbMBe"
   },
   "outputs": [],
   "source": [
    "save_model_to_drive(text_gen1, 'emb128_context5_relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ltv9dP7MQSAI"
   },
   "outputs": [],
   "source": [
    "text_gen1_loaded=load_model_from_drive('emb128_context5_relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wmuGYVts_gQ"
   },
   "source": [
    "## Visualization of Embeddings using t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4c2J69hOzv-"
   },
   "source": [
    "##### For visualization we are considering some nouns, pronouns, adverbs, verbs, synonyms, antonyms, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "tGgY1ROYDz7M"
   },
   "outputs": [],
   "source": [
    "tokens_to_plot=[\n",
    "                  'prince','lucca','pavlovna','anna','europe','crusades', #Names\n",
    "                  'who','where','when','what','which','why', # interrogative words\n",
    "                  'a', 'an', 'the', # articles\n",
    "                  'in', 'on', 'of', 'over', 'under', 'out', # prepositions\n",
    "                  'i','you','he','she','they', 'it', # pronouns\n",
    "                  # 'hot', 'cold', 'long', 'short', 'up', 'down', # antonyms\n",
    "                  'warn', 'caution', 'frightened', 'scared', 'importance','value', # synonymns\n",
    "                  # 'inevitably', 'urgently', 'apparently', 'constantly' # Adverbs\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "lmXAYBX022vl"
   },
   "outputs": [],
   "source": [
    "def plot_embeddings(tokens_to_plot, token_to_index, index_to_token, model):\n",
    "\n",
    "  embeds=np.array(model.embed(torch.tensor([token_to_index[token] for token in tokens_to_plot])).detach().numpy())\n",
    "\n",
    "  tsne = TSNE(n_components=2, perplexity=20, random_state=0)\n",
    "  embeds_2d = tsne.fit_transform(embeds)\n",
    "\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  plt.scatter(embeds_2d[:, 0], embeds_2d[:, 1])\n",
    "\n",
    "  for i, token in enumerate(tokens_to_plot):\n",
    "      plt.annotate(token, (embeds_2d[i, 0], embeds_2d[i, 1]))\n",
    "\n",
    "  plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
    "  plt.xlabel(\"t-SNE Component 1\")\n",
    "  plt.ylabel(\"t-SNE Component 2\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5yhRxdiOngr"
   },
   "outputs": [],
   "source": [
    "plot_embeddings(tokens_to_plot, token_to_index1, index_to_token1, text_gen1_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70XgeiFJw8g8"
   },
   "source": [
    "## next K words prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "sI6Gb2Em6ezz"
   },
   "outputs": [],
   "source": [
    "def get_embedding(word, vocab_words, embeddings):\n",
    "    if word in vocab_words:\n",
    "        idx = vocab_words.index(word)\n",
    "        return embeddings[idx].reshape(1, -1)\n",
    "    return np.mean(embeddings, axis=0).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "WiekOw9z6hBM"
   },
   "outputs": [],
   "source": [
    "def find_closest_word(avg_embed, vocab_embeds, vocab_words):\n",
    "    avg_embed=avg_embed.reshape(1,-1)\n",
    "    vocab_embeds = np.array(vocab_embeds).reshape(-1, avg_embed.shape[1])\n",
    "    similarities = cosine_similarity(avg_embed, vocab_embeds)\n",
    "    closest_idx = np.argmax(similarities)\n",
    "    closest_word = vocab_words[closest_idx]\n",
    "\n",
    "    return closest_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "qXSuT6QTMY_t"
   },
   "outputs": [],
   "source": [
    "# def predict_next_k_words(model, token_to_index, index_to_token, context, k):\n",
    "\n",
    "#     prompt_tokens = re.findall(r'\\b\\w+\\b|[.]', context)\n",
    "#     context_tokens=[]\n",
    "#     for token in prompt_tokens:\n",
    "#         if token in list(token_to_index.keys()):\n",
    "#             context_tokens.append(token)\n",
    "#         else:\n",
    "#             context_tokens.append(find_closest_word(token, list(token_to_index.keys()), np.array(list(token_to_index.values()))))\n",
    "\n",
    "#     context_indices = [token_to_index.get(word, token_to_index[' ']) for word in context_tokens]\n",
    "\n",
    "#     if len(context_indices) < context_len:\n",
    "#         context_indices = [1] * (context_len - len(context_indices)) + context_indices\n",
    "#     else:\n",
    "#         context_indices = context_indices[-context_len:]\n",
    "\n",
    "#     predicted_words = []\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for _ in range(k):\n",
    "#             context_tensor = torch.tensor(context_indices, dtype=torch.int64).unsqueeze(0)\n",
    "#             context_tensor=context_tensor.reshape(1,-1)\n",
    "#             output = model(context_tensor)\n",
    "#             next_word_index = torch.argmax(output, dim=1).item()\n",
    "#             next_word = index_to_token[next_word_index]\n",
    "#             predicted_words.append(next_word)\n",
    "#             context_indices.append(next_word_index)\n",
    "#             context_indices = context_indices[-context_len:]\n",
    "\n",
    "#     return ' '.join(predicted_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "BiVSIDxwRBFK"
   },
   "outputs": [],
   "source": [
    "def predict_next_k_words(context, k):\n",
    "    context_tokens = re.findall(r'\\b\\w+\\b|[.]', context)\n",
    "    context_indices = [token_to_index1.get(word, token_to_index1[' ']) for word in context_tokens]\n",
    "\n",
    "    # Trim or pad the context to fit the required length\n",
    "    if len(context_indices) < context_len:\n",
    "        context_indices = [1] * (context_len - len(context_indices)) + context_indices\n",
    "    else:\n",
    "        context_indices = context_indices[-context_len:]\n",
    "\n",
    "    predicted_words = []\n",
    "\n",
    "    text_gen1_loaded.eval()  # Set model to eval mode for inference\n",
    "    with torch.no_grad():\n",
    "        for _ in range(k):\n",
    "            # Convert context to tensor and pass through model\n",
    "            context_tensor = torch.tensor(context_indices, dtype=torch.int64).unsqueeze(0)\n",
    "            output = text_gen1_loaded(context_tensor)\n",
    "\n",
    "            # Get predicted word index and corresponding word\n",
    "            next_word_index = torch.argmax(output, dim=1).item()\n",
    "            next_word = index_to_token1[next_word_index]\n",
    "\n",
    "            # Add predicted word to results\n",
    "            predicted_words.append(next_word)\n",
    "\n",
    "            # Update context with new word and adjust to maintain context length\n",
    "            context_indices.append(next_word_index)\n",
    "            context_indices = context_indices[-context_len:]  # Keep only last `context_len` tokens\n",
    "\n",
    "    return ' '.join(predicted_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmPQDxqvONeE"
   },
   "outputs": [],
   "source": [
    "context = \"in the middle of the dreams she\"\n",
    "k = 100\n",
    "predicted_text = predict_next_k_words(context, k)\n",
    "print(predicted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PToH1AaEgRYO"
   },
   "outputs": [],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
